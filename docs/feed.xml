<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DyCon Platform</title>
    <description>This MATLAB software is dedicated to the development and learning of control problems for applied mathematics.</description>
    <link>https://DeustoTech.github.io/dycon-platform-documentation/</link>
    <atom:link href="https://DeustoTech.github.io/dycon-platform-documentation/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 25 Oct 2018 07:30:57 +0200</pubDate>
    <lastBuildDate>Thu, 25 Oct 2018 07:30:57 +0200</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Inverted Pendulum Control</title>
        <description>Init

clear
clc


syms t;
syms x dx theta dtheta;
syms M m l g;
syms Lgr Lgr_x Lgr_dx Lgr_theta Lgr_dtheta;


Define Lagrangian

Lgr = 1/2 * M*dx^2 + 2/3 * m*l^2*dtheta^2 + m*l*dx*dtheta*cos(theta)...
        + 1/2 *m*dx^2 - m*g*l*cos(theta);
Lgr_x = diff(Lgr,x);
Lgr_dx = diff(Lgr,dx);
Lgr_theta = diff(Lgr,theta);
Lgr_dtheta = diff(Lgr,dtheta);


Compute derivatives of Lagrangian _f means functionalized

syms x_f(t) dx_f(t) theta_f(t) dtheta_f(t) Lgr_x_f Lgr_dx_f Lgr_theta_f Lgr_dtheta_f;
dx_f(t)=diff(x_f,t);
dtheta_f=diff(theta_f,t);

Lgr_dx_f = subs(Lgr_dx,[x dx theta dtheta],[x_f dx_f theta_f dtheta_f])
Lgr_theta_f = subs(Lgr_theta,[x dx theta dtheta],[x_f dx_f theta_f dtheta_f])
Lgr_dtheta_f = subs(Lgr_dtheta,[x dx theta dtheta],[x_f dx_f theta_f dtheta_f])


 
Lgr_dx_f =
 
M*diff(x_f(t), t) + m*diff(x_f(t), t) + l*m*cos(theta_f(t))*diff(theta_f(t), t)
 
 
Lgr_theta_f =
 
g*l*m*sin(theta_f(t)) - l*m*sin(theta_f(t))*diff(theta_f(t), t)*diff(x_f(t), t)
 
 
Lgr_dtheta_f =
 
(4*m*diff(theta_f(t), t)*l^2)/3 + m*cos(theta_f(t))*diff(x_f(t), t)*l
 



Coumpute LHS of Lagrange equations of motion

syms Leq Eqn;
Leq = diff([Lgr_dx_f; Lgr_dtheta_f],t) - [Lgr_x; Lgr_theta_f];
syms Ddx Ddtheta u;
Leq = subs(Leq,[diff(x_f(t), t, t) , diff(theta_f(t), t, t) ],[Ddx, Ddtheta])-[u; 0];
Eqn = solve(Leq==0,[Ddx,Ddtheta]);


Construct state space equations

syms x1 x2 x3 x4 X;
X = [x1;x2;x3;x4];
subs(Eqn.Ddx, [x_f, diff(x_f(t),t), theta_f, diff(theta_f(t),t)], [x1, x2, x3, x4]);
subs(Eqn.Ddtheta, [x_f, diff(x_f(t),t), theta_f, diff(theta_f(t),t)], [x1, x2, x3, x4]);

syms Nsys
Nsys = [x2;...
        subs(Eqn.Ddx, [x_f, diff(x_f(t),t), theta_f, diff(theta_f(t),t)], [x1, x2, x3, x4]);...
        x4;...
        subs(Eqn.Ddtheta, [x_f, diff(x_f(t),t), theta_f, diff(theta_f(t),t)], [x1, x2, x3, x4])];

syms F;
F = subs(Nsys,u,0);
latex(F)



syms Amat Bmat
Amat = subs(jacobian(F,X),[x1,x2,x3,x4],[0,0,0,0]);
Bmat = diff(subs(Nsys,[x1,x2,x3,x4],[0,0,0,0]),u);


Physical parameters


  
    $m = 0.3 [kg]$;
  
  
    $M = 0.8 [kg]$;
  
  
    $l = 0.25 [m]$;
  
  
    $g = 9.8 [m/s^2]$;
  


A = double(subs(Amat,[m M l g],[0.3 0.8 0.25 9.8]));
B = double(subs(Bmat,[m M l g],[0.3 0.8 0.25 9.8]));


Control

Design an LQR controller

R = 1; Q = diag([1,1,1,1]);
[ricsol,cleig,K,report] = care(A,B,Q);

f_ctr = @(x) -K*x;

i_linear = @(t,x) A*x + B*f_ctr(x);

tspan = [0,10];
ini =[0,0,0.8,0]; %1.1894299
[time, state] = ode45(i_linear,tspan, ini);


Physical parameters

m = 0.3;% [kg];
M = 0.8;% [kg];
l = 0.25;% [m];
g = 9.8;% [m/s^2];
i_nonlinear = @(t,x)[                       x(2);                                                               ...
                    (4*f_ctr(x)-3*g*m*cos(x(3))*sin(x(3))+4*l*m*x(4)^2*sin(x(3)))/(4*M+4*m-3*m*cos(x(3))^2);    ...
                                            x(4);                                                               ...
                     -(3*(f_ctr(x)*cos(x(3))-g*m*sin(x(3))-M*g*sin(x(3))+l*m*x(4)^2*cos(x(3))*sin(x(3))))/(l*(4*M+4*m-3*m*cos(x(3))^2))];
 %
[timen, staten] = ode45(i_nonlinear,tspan, ini);


clf
plot(time, state(:,1),'LineWidth',2)
hold on
plot(timen, staten(:,1),'LineWidth',2)
grid on
legend('Linear', 'Nonlinear')
title('Cart position [m]')
hold off




[timen, staten] = ode45(i_nonlinear,tspan, ini);
clf
plot(time, state(:,3),'LineWidth',2)
hold on
plot(timen, staten(:,3),'LineWidth',2)
grid on
legend('Linear', 'Nonlinear')
title('Pndulum angle [rad]')




</description>
        <pubDate>Mon, 22 Oct 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp99/P0002-T</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp99/P0002-T</guid>
        
        
        <category>Tutorials</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Synchronized Oscillators</title>
        <description>What’s a consensus problem?

We consider a population of N coupled-phase oscillators, $\theta_i(t)$, $i=1,\ldots,N$ and we want to compute a control such that in time $T$ we reach the state $\theta_i(T)=\ldots =\theta_N(T)=:\bar{\theta}$ in which all the oscillators are sincronized.

The mathematical model

The dynamics of the oscillators is described by the Kuramoto model





The model is non-linear, then, we apply a standard linearization process around the steady state $\bar{\theta}$ obtaining



By classical techniques, the control is obtained by minimizing the following cost functional, subject to the linearized Kuramoto model, for each node



in which


  
    the first term measures the distance between the rest of nodes and the target.
  
  
    the second term is a penalization one introduced in order to   avoid using control with a too large size.
  


In fact, through the above minimization procedure, we find the control $b = (b_1,\ldots,b_N)$ which has minimum norm among all the controls capable to steer the system to the equilibrium dynamics.

How do you usually solve this type of problem?

The usual technique for solving minimizing problems based on quadratic functionals is the Gradient Descent Method, which is based on the following iterative algorithm



This technique is tipycally chosen because: - It is (relatively) easy to implement. - It is not very memory demanding.

What we propose?

As the functional J(b_i) depend on all nodes $\Theta_j$, the gradient $\nabla J(b_i)$ too. Therefore, in each iteration of the Gradient Descent algorithm we shall consider all nodes in the network. Then, when the number of nodes is large, the complexity if substantial per iteration. To avoid evaluate the full gradient, we propose to approach the problem by means of the Stochastic Descent Gradient method which only uses a small portion of data.

Matrix $A$ of the linearized Kuramoto model

N = 7;
A = (1/N)*ones(N,N);
for i = 1:N
    A(i,i) = -1;
end
A



A =

   -1.0000    0.1429    0.1429    0.1429    0.1429    0.1429    0.1429
    0.1429   -1.0000    0.1429    0.1429    0.1429    0.1429    0.1429
    0.1429    0.1429   -1.0000    0.1429    0.1429    0.1429    0.1429
    0.1429    0.1429    0.1429   -1.0000    0.1429    0.1429    0.1429
    0.1429    0.1429    0.1429    0.1429   -1.0000    0.1429    0.1429
    0.1429    0.1429    0.1429    0.1429    0.1429   -1.0000    0.1429
    0.1429    0.1429    0.1429    0.1429    0.1429    0.1429   -1.0000




Initial condition: 

mu = 4;sigma = 10;
theta0 = normrnd(mu,sigma,N,1)



theta0 =

   10.1450
   -3.8530
   12.2818
   10.6742
    2.7450
   -5.5123
   12.9204




Now, choose initial control,

t0 = 0;T = 1;dt = 0.1;
tspan = t0:dt:T;
u0 = zeros(length(tspan),N);


We can solve with the StochasticGradient function.

Results_Stochastic =  StochasticGradient(A,theta0,tspan,u0);


And see the convergence

fig1 = JPlotConvergence(Results_Stochastic,'Convergence of Stochastic Method');




We can see the result obtained

fig2 = LastThetaPlot(Results_Stochastic);




Comparison Clasical vs Stochastic

In this example, we show that stochastic method is faster than the classical descent in a small network.

Results_Classical = {};Results_Stochastic = {};
%
maxN = 80; % &amp;lt;=== Maximal number of oscillator allowed in the model
iter = 0;
for N = 1:15:maxN
    % We solve the problem for each N
    iter = iter + 1;
    % Definition of the linearized Kuramoto problem
    A = (1/N)*ones(N,N);
    for i = 1:N
        A(i,i) = -1;
    end
    % Initial state
    mu = 4;sigma = 5;
    theta0 = normrnd(mu,sigma,N,1);
    % Initial control
    t0 = 0;T = 1;dt = 0.1;
    tspan = t0:dt:T;
    u0 = zeros(length(tspan),N);

    % Classical Gradient Method
    Results_Classical{iter}  =  ClassicalGradient(A,theta0,tspan,u0);
    % Stochastic Gradient Method
    Results_Stochastic{iter} =  StochasticGradient(A,theta0,tspan,u0);
end


We can graphically see the following result. If the number of nodes increase we can see better the efectiveness of the stochastic gradient descent method with respect to the classical.

% For the next step it is necessary to convert the cells into arrays
Results_Classical  = [Results_Classical{:}];
Results_Stochastic = [Results_Stochastic{:}];
%
fig2 = figure; ax  = axes;
ax.FontSize = 17;
ax.XLabel.String = 'Number of Oscillators'; ax.YLabel.String = 'time(s)';
ax.XGrid = 'on'; ax.YGrid = 'on';
%
line([Results_Stochastic.N],[Results_Stochastic.t],'Parent',ax,'Color','red','Marker','s')
line([Results_Classical.N],[Results_Classical.t],'Parent',ax,'Color','blue','Marker','s')
legend({'Stochastic Gradient','Classical Gradient'})




Altough in this example we are dealing with a small network, we have shown that stochastic method is faster than the classical descent. If the number of nodes increase we can see better the efectiveness of the stochastic gradient descent method with respect to the classical. 

</description>
        <pubDate>Fri, 19 Oct 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp01/P0004-T</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp01/P0004-T</guid>
        
        
        <category>Tutorials</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>Gradient Descent in semilinear control</title>
        <description>We solve \begin{equation} \min_{u\in L^2((0,T)\times (0,\frac12))}J_{T}(u)=\frac12 \int_0^T\int_{0}^{0.5} u(t)^2 dxdt+\frac{1}{2}\int_0^T\int_{0}^{1} y(t,x)-z(x)^2 dxdt, \end{equation} where: \begin{equation} \begin{cases} y_t- y_{xx}+y^3=u\chi_{(0,\frac12)}\hspace{2.8 cm} &amp;amp; \mbox{in} \hspace{0.10 cm}(0,T)\times (0,1)\ y(t,0)=y(t,1)=0  &amp;amp; \mbox{on}\hspace{0.10 cm} (0,T)\ y(0,x)=y_0(x)  &amp;amp; \mbox{in}\hspace{0.10 cm}  (0,1). \end{cases} \end{equation} For time horizon $T$ large, one can check the emergence of the Turnpike property (see [1]).

%For further details about the problem, see e.g. [1], [2] or [3].


We employ a Gradient Descent Method.

STEP 1. We define the parameters for the algorithm.

% Number of points in the space partition.
%We count the boundary
Nx=100;

% Number of time steps
Nt=20000;

%WARNING: We have to fulfill
%Courant-Friedrichs-Levy condition:
%2(\Delta t)\leq (\Delta x)^2.

%Time horizon
T=2;

%Penalization parameter for the state.
beta=1;

%Initial datum for the state equation.
init=zeros(Nx,1);

%Target for the state.
zdiscr=ones(Nx,1);

% Maximum number of iterations
Nmax = 20000000;

% Stepsize for the Gradient Descent algorithm.
% appearing in the definition of the new iterate
% u = uold - delta*dJuold.
delta = 0.1;

% Tolerance
tol = 1e-1;


STEP 2. We initialize the algorithm.

% Control at time iter
u = zeros(Nt,floor(Nx/2));
% Control at time iter-1
uold = u;

%We compute the state yold corresponding to the control uold

%The multiplication by
%the characteristic function \chi_{(0,0.5)}
%is included
%in the definition of &quot;source&quot;.
source=zeros(Nt,Nx);
for k=1:Nt
    for i=1:floor(Nx/2)
        source(k,i)=u(k,i);
    end
end

[ y ] = heat_semilinear( @(x) x.^3, T, init, source );

%We compute the adjoint state
%corresponding to control uiter.

%We define the matrix defining
%the discretized version of
%the source:
%\beta(y(t,x)-z(x)).

source=zeros(Nt,Nx);
for k=1:Nt
    for i=1:Nx
        source(k,i)=beta*(y(k,i)-zdiscr(i,1));
    end
end

[ p ] = adjoint_heat_semilinear( y, T, source );

prestr=p(:,1:floor(Nx/2));
prestrold=prestr;

% Initial error
error = 10;
% Iteration counter
iter = 0;


STEP 3. We set up a while loop for the Gradient Descent method.

while (error &amp;gt; tol &amp;amp;&amp;amp; iter &amp;lt; Nmax)
    % Update iteration counter
    iter = iter + 1;

    % Gradient computed at u_{old}
    dJuold = uold+prestrold;
    % Update control
    u = uold - delta*dJuold;

    %We compute the state y_{iter} corresponding to the control uiter

    %In the definition of &quot;source&quot;
    %it is included the multiplication by
    %the characteristic function \chi_{(0,0.5)}.
    source=zeros(Nt,Nx);
    for k=1:Nt
        for i=1:floor(Nx/2)
            source(k,i)=u(k,i);
        end
    end

    [ y ] = heat_semilinear( @(x) x.^3, T, init, source );

    %We compute the adjoint state
    %corresponding to control uiter.

    %We define the matrix defining
    %the discretized version of
    %the source:
    %\beta(y(t,x)-z(x)).

    source=zeros(Nt,Nx);
    for k=1:Nt
        for i=1:Nx
            source(k,i)=beta*(y(k,i)-zdiscr(i,1));
        end
    end

    [ p ] = adjoint_heat_semilinear( y, T, source );

    prestr=p(:,1:floor(Nx/2));
    prestrold=prestr;

    %old control
    uold=u;

    % Control update norm
    dJuold2 = sum(sum(dJuold.^2))*(T/(Nt-1))*(1/(Nx-1));
    u2 = sum(sum(u.^2))*(T/(Nt-1))*(1/(Nx-1));

    if (u2 == 0)
        error=sqrt(dJuold2);
    else
        error=sqrt(dJuold2/u2);
    end

    %we compute the state term of the fucntional.
    stateterm=0;
    for k=1:Nt
        stateterm=stateterm+sum((transpose(y(k,:))-zdiscr).^2)*(1/(Nx-1))*(T/(Nt-1));
    end
    Ju=(0.5)*u2+(beta/2)*stateterm;

    fprintf(&quot;Iteration %i - Error %g - Cost %g\n&quot;, iter, error, Ju);

end


STEP 4. Optimal control and optimal state.

%approximate optimal control.
uopt=u;
%approximate optimal state.
yopt=y;


STEP 5. We plot optimal control and optimal state.

%optimal control.
figure(1)
clf(1);
timecontrol=linspace(0,T,Nt);
spacecontrol=linspace(0,1,floor(Nx/2));
[TIMECONTROL,SPACECONTROL]=meshgrid(timecontrol,spacecontrol);
surf(TIMECONTROL,SPACECONTROL,transpose(uopt),'EdgeColor','none');
colormap jet;
title('optimal control');
xlabel('t [time]','FontSize',20);
ylabel('x [space]','FontSize',20);
zlabel('u [control]','FontSize',20);
xt=get(gca,'XTick');
set(gca,'FontSize',20);

%optimal state.
figure(2);
clf(2);
time=linspace(0,T,Nt);
space=linspace(0,1,Nx);
[TIME,SPACE]=meshgrid(time,space);
surf(TIME,SPACE,transpose(yopt),'EdgeColor','none');
colormap jet;
title('optimal state');
xlabel('t [time]','FontSize',20);
ylabel('x [space]','FontSize',20);
zlabel('y [state]','FontSize',20);
xt=get(gca,'XTick');
set(gca,'FontSize',20);


References

%[1] Porretta, Alessio and Zuazua, Enrique, Remarks on long time versus steady state optimal control, Mathematical Paradigms of Climate Science, Springer, 2016, pp. 67--89.

%[2] Casas, Eduardo and Mateos, Mariano, Optimal control of partial differential equations,
%    Computational mathematics, numerical analysis and applications,
%    Springer, Cham, 2017, pp. 3--5.

%[3] Tr{\&quot;o}ltzsch, Fredi, Optimal control of partial differential equations, Graduate studies in mathematics, American Mathematical Society, 2010.


</description>
        <pubDate>Thu, 18 Oct 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp02/P0001-T</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp02/P0001-T</guid>
        
        
        <category>Tutorials</category>
        
        <category>WP02</category>
        
      </item>
    
      <item>
        <title>Control of PDEs involving non-local terms</title>
        <description>Why non-local?

Relevant models in Continuum Mechanics, Mathematical Physics and Biology are of non-local nature:

  Boltzmann equations in gas dynamics;
  Navier-Stokes equations in Fluid Mechanics;
  Keller-Segel model for Chemotaxis.


Moreover, these models are applied for the description of several complex phenomena for which a local approach is inappropriate or limiting.

  Peierls-Nabarro equation in elasticity 1
  Image processing algorithms 2
  Anomalous diffusion models 3
  Finance: description of the pricing of American options 4.


In this setting, classical PDE theory fails because of non-locality. Yet many of the existing techniques can be tuned and adapted, although this is often a delicate matter.

Fractional time derivatives

We analyze the problem of controllability of fractional (in time) Partial Differential Equations. In contrast with the classical PDE control theory, when driving these systems to rest, one is required not only to control the value of the state at the final time but also the memory accumulated by the long-tail effects that the fractional derivative introduces.
As a consequence, the notion of null controllability to equilibrium needs to take into account both the state and the memory term.

In particular, in (5) we consider the full controllability problem for the system



with $A$ and $B$ two linear unbound operators and where $\partial_{t,0+}^{\alpha}$ is a classical Caputo derivative. We show that, due to the memory effects controllability cannot be achieved in finite time.

Fractional Schrödinger and wave equations

In 6, we analyse evolution problems involving the fractional Laplace operator $(-\Delta)^s$, $s\in(0,1)$, on a bounded $C^{1,1}$ domain $\Omega\subset\mathbb{R}^N$. In particular we consider the fractional Schrödinger equation



and the fractional wave equation



Our main goal is to study the controllability of this kind of phenomena.


  Fractional Schrödinger equation: we assume Dirichlet homogeneous boundary conditions and we prove null controllability provided $s\geq 1/2$ and that the control is active on a neighborhood $\omega$ of a subset of the boundary fulfilling the classical multiplier conditions. Moreover, in the limit case $s=1/2$, controllability holds only if the control time $T$ is large enough.
  Fractional wave equation: we obtain analogous controllability properties, as a direct consequence of the results for the Schrödinger equation.


These mentioned controllability properties may be studied also through the employment of microlocal analysis techniques. In this framework, in 7 we present a WKB expansion for the fractional Schrödinger equaiton in one space dimension, which allows to show that the solutions of the model propagate following the rays of geometric optics and that their controllability properties may be reinterpreted in terms of the velocity of propagation of this mentioned rays.

Viscoelasticity

Viscoelastic materials are those for which the behaviour combines liquid-like and solid-like characteristic 8.







In 9, we consider the following model of viscoelasticity, given by a wave equation with both a viscous Kelvin-Voigt and frictional damping



in which we incorporate an internal control $h$ with a moving support. Our analysis is based on the fact that the above equation can be rewritten as a system coupling a parabolic equation with an ODE. The presence of this ODE, in the case of a fixed  support of the control, is responsible for the lack of controllability of the system, due to the absence of propagation in the space-like direction. Therefore, we prove the null controllability when the control region, driven by the flow of an ODE, covers all the domain.

Equations with non-local spatial terms

In 10, we study the null controllability of the following linear heat equation with spatial non-local integral terms:



Under some analyticity assumptions on the corresponding kernel, we show that the equations is controllable. We employ compactness-uniqueness arguments in a suitable functional setting, an argument that is harder to apply for heat equations because of its very strong time irreversibility.

These mentioned results have been later improved in [], where we adopted a Carleman approach which allowed us to remove the (strong) analitycity assumptions on the kernel, and to replace them with sharp exponential decay conditions at the extrema of the time domain considered.

Models with memory terms

In 11, we approach the control problem for the following heat equation with lower order memory



This can be rewritten as a system coupling a heat equation with an ordinary differential equation, as in the context of viscoelasticity. In view of this structure we introduce a Moving geometric Control Condition (MGCC), which turns out to be sufficient for moving control. Furthermore, in (10) we obtain similar results for the following wave equation with a lower order memory term



while in 7 we were able to address the case of the following one-dimensional wave equation with memory in the principal part


Perspectives


  Weakening the MGCC for the control of viscoelasticity models.
  Models with fractional time derivatives: what kind of control theoretical properties can be expected once exact controllability is excluded?
  General analytic memory kernels.
  Geometric Optics for wave-like models involving the fractional Laplacian.
  Can Carleman inequalities handle non-local terms?
  Links with delay systems.
  Nonlinear models.


References


  
    
      G.~Lu - “The Peierls-Nabarro model of dislocations: a venerable theory and its current development” - Handbook of Materials Modeling, 2005 &amp;#8617;
    
    
      G.~Gilboa and S.~Osher - “Nonlocal operators with applications to image processing” - Multiscale Model Simul., 2008 &amp;#8617;
    
    
      M.~Bologna, C.~Tsallis and P. Grigolini - “Anomalous diffusion associated with nonlinear fractional derivative Fokker-Plank-like equation: exact time-time dependent solutions” - Phys. Rev., 2000 &amp;#8617;
    
    
      S.~Levendorski - “Pricing of the American put under Levy processes” - Int. J. Theor. Appl. Finance, 2004 &amp;#8617;
    
    
      Q.~L&quot;u and E. Zuazua - “On the lack of controllability of fractional in time ODE and PDE” - Math. Control Signals Syst., 2016 &amp;#8617;
    
    
      U.~Biccari - “Internal control for non-local Schr&quot;odinger and wave eqautions involving the fractional Laplace operator” - Submitted &amp;#8617;
    
    
      U.~Biccari, and Aceves, A. B. WKB expansion for a fractional Schröodinger equation with applications to controllability. Submitted. (2018). &amp;#8617; &amp;#8617;2
    
    
      T.H.~Banks, S.~Hu and Z.R.~Kenz - “A brief review of elasticity and viscoelasticity for solids} - Adv. Appl. Math. Mech., 2011.” &amp;#8617;
    
    
      F.~Chaves, L.~Rosier and E.~Zuazua - “Null controllability of a system of viscoelasticity with moving control” - J. Math. Pures Appl., 2014. &amp;#8617;
    
    
      Q.~L&quot;u, X.~Zhang and E. Zuazua - “Null controllability of wave equations with memory” - Submitted &amp;#8617; &amp;#8617;2
    
    
      F.~Chaves, X.~Zhang and E.~Zuazua - “Controllability of evolution equations with memory” - Submitted &amp;#8617;
    
  

</description>
        <pubDate>Mon, 01 Oct 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/posts/Control_of_PDE_non_local_terms</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/posts/Control_of_PDE_non_local_terms</guid>
        
        
        <category>Posts</category>
        
      </item>
    
      <item>
        <title>Rays propagation of a fractional Schrodinger Equation</title>
        <description>Shows the propagation of the solution of a fractional Schrodinger equation with concentrated and highly oscillatory initial datum. The solution remains concentrated along the rays of geometric optics

N = 250;
L = 1;
hx = (2*L)/(N+1);


Definition of the initial datum u0 as a function_handle. u0 is chosen as a Gaussian profile multiplied by a higly oscillatory function

x0 = 0; % Center of the Gaussian profile
gamma = hx^(-0.9); % Amplitude of the Gaussian profile
fr = (1/hx)*pi^2/16; % Frequency of the oscillations


u0 = @(x) exp(-0.5*gamma*(x-x0).^2).*exp(1i*fr*x);


Plot of the initial datum

fig = gcf;
set(gcf,'Units','pixels','Position',[427 306 712 284])

x = -L:hx:L;

subplot(1,3,1) % Modulus
plot(x,abs(u0(x)))
title('|u_0(x)|')
xlabel('x'); ylabel('u(x)');

subplot(1,3,2) % Real part
plot(x,real(u0(x)))
title('real(u_0(x))')
xlabel('x'); ylabel('u(x)');

subplot(1,3,3) % Imaginary part
plot(x,imag(u0(x)))
title('img(u_0(x))')
xlabel('x'); ylabel('u(x)');

format_plot(fig)




Solution for s = 1/2

Define the characteristic parameters of the problem

s = 0.5  % Order of the fractional Laplacian
L        % Extrema of the space interval
N        % Number of points in the space mesh
T = 5    % Length of the time interval
u0       % The function_handle that we have showed before.



s =

    0.5000


L =

     1


N =

   250


T =

     5


u0 =

  function_handle with value:

    @(x)exp(-0.5*gamma*(x-x0).^2).*exp(1i*fr*x)




To solve the equation, we call the function fractional_schr. The solution of the equation is stored in the u variable.

[x,t,u] = fractional_schr(s,L,N,T,u0);


Now we can see a graphical interpretation

[X,T] = meshgrid(x,t);
%
clf
mesh(X,T,u');
format_plot(gcf);view(0,90)
xlabel('x'); ylabel('t'); title('Ray Evolution');




By typing “animation(x,t,u)” in the MATLAB console you can see the evolution in time of this wave.



</description>
        <pubDate>Sat, 21 Jul 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp05/P0001-T</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp05/P0001-T</guid>
        
        
        <category>Tutorials</category>
        
        <category>WP05</category>
        
      </item>
    
      <item>
        <title>Average Control by classical gradient step method</title>
        <description>In this work, we address the optimal control of parameter-dependent systems. We introduce the notion of averaged control in which the quantity of interest is the average of the states with respect to the parameter family 

In this case $\nu_i$ are:

nu = 1:0.5:6


And save in K, the number of values

K = length(nu);


Where the finite dimensional linear control system is:



We can, define the initial state of all ode’s

N = 3; % dimension of vector state
x0 = ones(N, 1);


Also, need define a initial control, that will be evolve

dt = 0.02;
t0 = 0; T  = 1;
span = (t0:dt:T);
%
u = zeros(length(span),1);


%Moreover, we can define the matrix A's and B's, that determine the problem
Am = -triu(ones(N))


Bm = zeros(N, 1);
Bm(N) = 1


So, we can create these edo’s in variable primal_odes.

primal_odes = zeros(1,K,'ode');
for index = 1:K
    A = Am + (nu(index) - 1 )*diag(diag(Am));
    %
    primal_odes(index) = ode(A,'B',Bm);
    % all have the same control
    primal_odes(index).u  = u;
    % time intervals
    primal_odes(index).span = span;
    % initial state
    primal_odes(index).x0 = x0;
end


So, we have a $K$ ordinary differential equations

primal_odes


To solve average control problem to x0; in this case:

xt = ones(N, 1)


we can solve the minimization problem



We can use the classical gradient descent method based on the adjoint methodology, and obtain the corresponding adjoint system for 1,



The same way that before, we define the adjoints problems

adjoint_odes = zeros(1,K,'ode');
for index = 1:K
    A = primal_odes(index).A';
    adjoint_odes(index) = ode(A);
    % all have the same control
    adjoint_odes(index).u = u;
    % time intervals
    adjoint_odes(index).span = span;
end


However the initial state  adjoint_odes(index).x0 hasn’t been assign. This initial state will be assign in every step of solution.

To minimize the functional, $\mathcal{J}\left( u\right)$, we take the steepest descent direction given by



We process to solve the problem of classical gradient descent

gamma = 1;
beta  = 1e-3;
tol   = 1e-8;  % Tolerance
error = Inf;
MaxIter = 50;
iter = 0;
xhistory = {}; uhistory = {};  error_history = [];    % array here we will save the evolution of average vector states
while (error &amp;gt; tol &amp;amp;&amp;amp; iter &amp;lt; MaxIter)
    iter = iter + 1;
    % solve primal problem
    % ====================
    solve(primal_odes);
    % calculate mean state final vector of primal problems
    xMend = forall({primal_odes.xend},'mean');

    % solve adjoints problems
    % =======================
    % update new initial state of all adjoint problems
    for iode = adjoint_odes
        iode.x0 = -(xMend' - xt);
    end
    % solve adjoints problems with the new initial state
    solve(adjoint_odes);

    % update control
    % ===============
    % calculate mean state vector of adjoints problems
    pM = forall({adjoint_odes.x},'mean');
    pM = pM*Bm;

    % reverse adjoint variable
    pM = flipud(pM);
    % Control update
    u = primal_odes(1).u; % catch control currently
    Du = beta*u - pM;
    u = u - gamma*Du;
    % update control in primal problems
    for index = 1:K
        primal_odes(index).u = u;
    end
    % Control error
    % =============
    % Calculate area ratio  of Du^2 and u^2
    Au2   =  trapz(span,u.^2);
    ADu2  =  trapz(span,Du.^2);
    %

    error = sqrt(ADu2/Au2);
    % Save evolution
    xhistory{iter} = [ span',forall({primal_odes.x},'mean')];
    uhistory{iter} = [ span',u];
    error_history  = [ error_history, error];
end


The average control obtain is

plot(span,u)
xlabel('time');ylabel('u(t)')
format_plot(gcf)


Also, on average the objective [0 0 0] has been reached.

figure;
plot(iode.span,forall({primal_odes.x},'mean'))
xlabel('t');ylabel('x_{i}(t)')
legend(strcat('x_{',num2str((1:N)','%0.1d'),'}(t)'))
title('Evolution of cordinates of vector state.')
format_plot(gcf)


You can use the comand

animation_sol(xhistory,uhistory,'XLim',[-0.1 0.25],'ULim',[-0.5 0.0])

We can see 

If we analyze the evolution in the error, we can see that we should have stopped, in iteration 20.

plot(error_history,'-*')
title('Error Evolution')
ylabel('Error'); xlabel('Iterations')
format_plot(gcf)


References


  
    
      E. Zuazua (2014) Averaged Control. Automatica, 50 (12), p. 3077-3087. &amp;#8617;
    
  

</description>
        <pubDate>Sat, 21 Jul 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp01/P0001-T</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp01/P0001-T</guid>
        
        
        <category>Tutorials</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>rigidity_fr_laplacian</title>
        <description>

Description

Summary of example objective

Section 1 Title

Description of first code block

a = 1;


Section 2 Title

Description of second code block

b = 2;


</description>
        <pubDate>Sat, 01 Apr 2017 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/functions/rigidity_fr_laplacian-F</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/functions/rigidity_fr_laplacian-F</guid>
        
        
        <category>Functions</category>
        
      </item>
    
      <item>
        <title>fractional_schr</title>
        <description>

Description

Solves the fractional Schrodinger equation



using FE for the approximation of the fractional Laplacian and Cranck-Nicholson for the time integration

For this example we choose following parameters

s = 0.5;
N = 100;
L = 1;


Execute the function

A = rigidity_fr_laplacian(s,L,N);


Can see graphically representation of matrix

figure(1)
mesh(A)
view(155,100)




</description>
        <pubDate>Sat, 01 Apr 2017 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/functions/fractional_schr-F</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/functions/fractional_schr-F</guid>
        
        
        <category>Functions</category>
        
      </item>
    
  </channel>
</rss>
