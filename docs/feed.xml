<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DyCon Platform</title>
    <description>This MATLAB software is dedicated to the development and learning of control problems for applied mathematics.</description>
    <link>https://DeustoTech.github.io/dycon-platform-documentation/</link>
    <atom:link href="https://DeustoTech.github.io/dycon-platform-documentation/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 26 Oct 2018 09:57:47 +0200</pubDate>
    <lastBuildDate>Fri, 26 Oct 2018 09:57:47 +0200</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>Synchronized Oscillators</title>
        <description>What‚Äôs a consensus problem?

We consider a population of N coupled-phase oscillators, $\theta_i(t)$, $i=1,\ldots,N$ and we want to compute a control such that in time $T$ we reach the state $\theta_i(T)=\ldots =\theta_N(T)=:\bar{\theta}$ in which all the oscillators are sincronized.

The mathematical model

The dynamics of the oscillators is described by the Kuramoto model





The model is non-linear, then, we apply a standard linearization process around the steady state $\bar{\theta}$ obtaining



By classical techniques, the control is obtained by minimizing the following cost functional, subject to the linearized Kuramoto model, for each node



in which


  
    the first term measures the distance between the rest of nodes and the target.
  
  
    the second term is a penalization one introduced in order to   avoid using control with a too large size.
  


In fact, through the above minimization procedure, we find the control $b = (b_1,\ldots,b_N)$ which has minimum norm among all the controls capable to steer the system to the equilibrium dynamics.

How do you usually solve this type of problem?

The usual technique for solving minimizing problems based on quadratic functionals is the Gradient Descent Method, which is based on the following iterative algorithm



This technique is tipycally chosen because: - It is (relatively) easy to implement. - It is not very memory demanding.

What we propose?

As the functional J(b_i) depend on all nodes $\Theta_j$, the gradient $\nabla J(b_i)$ too. Therefore, in each iteration of the Gradient Descent algorithm we shall consider all nodes in the network. Then, when the number of nodes is large, the complexity if substantial per iteration. To avoid evaluate the full gradient, we propose to approach the problem by means of the Stochastic Descent Gradient method which only uses a small portion of data.

Matrix $A$ of the linearized Kuramoto model

N = 7;
A = (1/N)*ones(N,N);
for i = 1:N
    A(i,i) = -1;
end
A



A =

   -1.0000    0.1429    0.1429    0.1429    0.1429    0.1429    0.1429
    0.1429   -1.0000    0.1429    0.1429    0.1429    0.1429    0.1429
    0.1429    0.1429   -1.0000    0.1429    0.1429    0.1429    0.1429
    0.1429    0.1429    0.1429   -1.0000    0.1429    0.1429    0.1429
    0.1429    0.1429    0.1429    0.1429   -1.0000    0.1429    0.1429
    0.1429    0.1429    0.1429    0.1429    0.1429   -1.0000    0.1429
    0.1429    0.1429    0.1429    0.1429    0.1429    0.1429   -1.0000




Initial condition: 

mu = 4;sigma = 10;
theta0 = normrnd(mu,sigma,N,1)



theta0 =

   20.3206
  -11.3219
   -9.3685
  -10.7385
    3.5834
   -2.1551
   17.1415




Now, choose initial control,

t0 = 0;T = 1;dt = 0.1;
tspan = t0:dt:T;
u0 = zeros(length(tspan),N);


We can solve with the StochasticGradient function.

Results_Stochastic =  StochasticGradient(A,theta0,tspan,u0);


And see the convergence

fig1 = JPlotConvergence(Results_Stochastic,'Convergence of Stochastic Method');




We can see the result obtained

fig2 = LastThetaPlot(Results_Stochastic);




Comparison Clasical vs Stochastic

In this example, we show that stochastic method is faster than the classical descent in a small network.

Results_Classical = {};Results_Stochastic = {};
%
maxN = 500; % &amp;lt;=== Maximal number of oscillator allowed in the model
iter = 0;
for N = 1:100:maxN
    % We solve the problem for each N
    iter = iter + 1;
    % Definition of the linearized Kuramoto problem
    A = (1/N)*ones(N,N);
    for i = 1:N
        A(i,i) = -1;
    end
    % Initial state
    mu = 4;sigma = 5;
    theta0 = normrnd(mu,sigma,N,1);
    % Initial control
    t0 = 0;T = 1;dt = 0.1;
    tspan = t0:dt:T;
    u0 = zeros(length(tspan),N);

    % Classical Gradient Method
    Results_Classical{iter}  =  ClassicalGradient(A,theta0,tspan,u0);
    % Stochastic Gradient Method
    Results_Stochastic{iter} =  StochasticGradient(A,theta0,tspan,u0);
end


We can graphically see the following result. If the number of nodes increase we can see better the efectiveness of the stochastic gradient descent method with respect to the classical.

% For the next step it is necessary to convert the cells into arrays
Results_Classical  = [Results_Classical{:}];
Results_Stochastic = [Results_Stochastic{:}];
%
fig2 = figure; ax  = axes;
ax.FontSize = 17;
ax.XLabel.String = 'Number of Oscillators'; ax.YLabel.String = 'time(s)';
ax.XGrid = 'on'; ax.YGrid = 'on';
%
line([Results_Stochastic.N],[Results_Stochastic.t],'Parent',ax,'Color','red','Marker','s')
line([Results_Classical.N],[Results_Classical.t],'Parent',ax,'Color','blue','Marker','s')
legend({'Stochastic Gradient','Classical Gradient'})




Altough in this example we are dealing with a small network, we have shown that stochastic method is faster than the classical descent. If the number of nodes increase we can see better the efectiveness of the stochastic gradient descent method with respect to the classical. 

</description>
        <pubDate>Fri, 19 Oct 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp01/P0004-T</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp01/P0004-T</guid>
        
        
        <category>Tutorials</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>Gradient Descent in semilinear control</title>
        <description>We solve



where:



The fucntional $J_T$ is made of two addenda. The first one penalizes the control, while the second one is a tracking term penalizing the distance between the state and the target. Our goal is to keep the state near the target for all times, by a cheap control. As $\beta$ increases, the distance between the optimal state and the target decreases.

For time horizon $T$ large, one can check the emergence of the Turnpike property (see 1). For further details about the problem, see e.g. 1, 2 or 3.

We employ a Gradient Descent Method.

STEP 1. We define the parameters for the algorithm.

Number of points in the space partition.

%We count the boundary
Nx=50;


Number of time steps

Nt=200;


WARNING: We have to fulfill Courant-Friedrichs-Levy condition:



Time horizon

T=2;


Penalization parameter for the state.

beta=1000;


Initial datum for the state equation.

init=zeros(Nx,1);


Target for the state.

zdiscr=ones(Nx,1);


Maximum number of iterations

Nmax = 100;


Stepsize for the Gradient Descent algorithm. appearing in the definition of the new iterate u = uold - delta*dJuold.

delta = 0.1;


Tolerance

tol = 1e-1;


STEP 2. We initialize the algorithm.

Control at time iter

u = zeros(Nt,floor(Nx/2));


Control at time iter-1

uold = u;


We compute the state yold corresponding to the control uold. The multiplication by the characteristic function $\chi_{(0,0.5)}$ is included in the definition of ‚Äúsource‚Äù.

source=zeros(Nt,Nx);
for k=1:Nt
    for i=1:floor(Nx/2)
        source(k,i)=u(k,i);
    end
end

[ y ] = heat_semilinear( @(x) x.^3, T, init, source );


We compute the adjoint state corresponding to control uiter. We define the matrix defining the discretized version of the source: $\beta(y(t,x)-z(x))$.

source=zeros(Nt,Nx);
for k=1:Nt
    for i=1:Nx
        source(k,i)=beta*(y(k,i)-zdiscr(i,1));
    end
end

[ p ] = adjoint_heat_semilinear( y, T, source );

prestr=p(:,1:floor(Nx/2));
prestrold=prestr;


% Initial error
error = 10;
% Iteration counter
iter = 0;


STEP 3. We set up a while loop for the Gradient Descent method.

while (error &amp;gt; tol &amp;amp;amp;&amp;amp;amp; iter &amp;lt; Nmax)
    % Update iteration counter
    iter = iter + 1;

    % Gradient computed at u_{old}
    dJuold = uold+prestrold;
    % Update control
    u = uold - delta*dJuold;

    %We compute the state y_{iter} corresponding to the control uiter

    %In the definition of &quot;source&quot;
    %it is included the multiplication by
    %the characteristic function \chi_{(0,0.5)}.
    source=zeros(Nt,Nx);
    for k=1:Nt
        for i=1:floor(Nx/2)
            source(k,i)=u(k,i);
        end
    end

    [ y ] = heat_semilinear( @(x) x.^3, T, init, source );

    %We compute the adjoint state
    %corresponding to control uiter.

    %We define the matrix defining
    %the discretized version of
    %the source:
    %\beta(y(t,x)-z(x)).

    source=zeros(Nt,Nx);
    for k=1:Nt
        for i=1:Nx
            source(k,i)=beta*(y(k,i)-zdiscr(i,1));
        end
    end

    [ p ] = adjoint_heat_semilinear( y, T, source );

    prestr=p(:,1:floor(Nx/2));
    prestrold=prestr;

    %old control
    uold=u;

    % Control update norm
    dJuold2 = sum(sum(dJuold.^2))*(T/(Nt-1))*(1/(Nx-1));
    u2 = sum(sum(u.^2))*(T/(Nt-1))*(1/(Nx-1));

    if (u2 == 0)
        error=sqrt(dJuold2);
    else
        error=sqrt(dJuold2/u2);
    end

    %we compute the state term of the fucntional.
    stateterm=0;
    for k=1:Nt
        stateterm=stateterm+sum((transpose(y(k,:))-zdiscr).^2)*(1/(Nx-1))*(T/(Nt-1));
    end
    Ju=(0.5)*u2+(beta/2)*stateterm;

    fprintf(&quot;Iteration %i - Error %g - Cost %g\n&quot;, iter, error, Ju);

end


Iteration 1 - Error 10 - Cost 503.646
Iteration 2 - Error 2.69873 - Cost 429.651
Iteration 3 - Error 1.0966 - Cost 414.035
Iteration 4 - Error 0.552452 - Cost 409.793
Iteration 5 - Error 0.320747 - Cost 408.451
Iteration 6 - Error 0.21037 - Cost 407.986
Iteration 7 - Error 0.152608 - Cost 407.827
Iteration 8 - Error 0.118849 - Cost 407.792
Iteration 9 - Error 0.0967289 - Cost 407.812



STEP 4. Optimal control and optimal state.

%approximate optimal control.
uopt=u;
%approximate optimal state.
yopt=y;


STEP 5. We plot optimal control and optimal state.

Optimal control.

figure(1)
clf(1);
timecontrol=linspace(0,T,Nt);
spacecontrol=linspace(0,1,floor(Nx/2));
[TIMECONTROL,SPACECONTROL]=meshgrid(timecontrol,spacecontrol);
surf(TIMECONTROL,SPACECONTROL,transpose(uopt),'EdgeColor','none');
colormap jet;
%title('optimal control');
xlabel('t [time]','FontSize',20);
ylabel('x [space]','FontSize',20);
zlabel('u [control]','FontSize',20);
xt=get(gca,'XTick');
set(gca,'FontSize',20);




Optimal state.

figure(2);
clf(2);
time=linspace(0,T,Nt);
space=linspace(0,1,Nx);
[TIME,SPACE]=meshgrid(time,space);
surf(TIME,SPACE,transpose(yopt),'EdgeColor','none');
colormap jet;
%title('optimal state');
xlabel('t [time]','FontSize',20);
ylabel('x [space]','FontSize',20);
zlabel('y [state]','FontSize',20);
xt=get(gca,'XTick');
set(gca,'FontSize',20);




References


  
    
      Porretta, Alessio and Zuazua, Enrique, Remarks on long time versus steady state optimal control, Mathematical Paradigms of Climate Science, Springer, 2016, pp. 67‚Äì89¬†&amp;#8617;¬†&amp;#8617;2
    
    
      Casas, Eduardo and Mateos, Mariano, Optimal control of partial differential equations,    Computational mathematics, numerical analysis and applications,    Springer, Cham, 2017, pp. 3‚Äì5.¬†&amp;#8617;
    
    
      Troltzsch, Fredi, Optimal control of partial differential equations, Graduate studies in mathematics, American Mathematical Society, 2010.¬†&amp;#8617;
    
  

</description>
        <pubDate>Thu, 18 Oct 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp02/P0001-T</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp02/P0001-T</guid>
        
        
        <category>Tutorials</category>
        
        <category>WP02</category>
        
      </item>
    
      <item>
        <title>WKB expansion for a fractional Schr√∂dinger equation with applications to controllability</title>
        <description>In recent years, the study of fractional integro-differential equations applied to physics and other areas has faced an extensive growth. In this framework, in the early 2000 Laskin started considering extensions of the classical quantum mechanics theory, based on the idea of replacing the classical Brownian trajectories in Feynman path integrals by Levy flights, which are generated by fractional Laplacians (). This gave birth to Fractional Quantum Mechanics (FQM), which is the theory of quantum mechanics based on the fractional Schr√∂dinger equation (FSE), instead than on the classical integer one.
Later on, these kind of models found applications in several branches of physics, such as the study of condensed-matter realizations of L√©vy cristals (), lasers implementation (), acoustic wave equations (), gravity waves (), geophysics ().
The prototypical example of a fractional Schr√∂dinger equation is given by the following one-dimensional non-local partial differential equation $$\begin{aligned}
\label{main_eq}
    {\mathcal{P} }_s u:= \left[i\partial_t + {(-d_x^{\,2})^{s} }\right]u = 0, &amp;amp;\;\;  (x,t)\in{\mathbb{R} }\times(0,+\infty), \end{aligned}$$ in which (‚ÄÖ‚àí‚ÄÖdx‚ÄÜ2)s is the fractional Laplacian, defined for all s‚ÄÑ‚àà‚ÄÑ(0,‚ÄÜ1) and for any function f sufficiently smooth as the following singular integral $$\begin{aligned}
{(-d_x^{\,2})^{s}f}(x):={c_{1,s} }\; P.V. \int_{ {\mathbb{R} }}\frac{f(x)-f(y)}{ {|x-y|^{1+2s} }}\,dy,\end{aligned}$$ with c1,‚ÄÜs a normalization constant given by $$\begin{aligned}
    {c_{1,s} }:= \left(\int_{ {\mathbb{R} }} \frac{1-\cos(z)}{|z|^{1+2s} }\,dz\right)^{-1} = \frac{s2^{2s}\Gamma\left(s+\frac 12\right)}{\sqrt{\pi}\Gamma(1-s)},\end{aligned}$$ where Œì is the usual Gamma function.
In , we developed a study of the propagation properties of the solutions of , based on a WKB analysis of the equation starting from a highly oscillatory initial datum in the form $$\begin{aligned}
\label{in_dat}
    u(x,0) = {u_{\textrm{\small in} }}(x) e^{i\frac{\xi_0}{\varepsilon} x}:=u_0(x),\;\;\; \xi_0\in{\mathbb{R} }.\end{aligned}$$
Here, the parameter Œµ represents the fast space and time scale introduced in the equation, as well as the typical wavelength of oscillations of the initial datum.
Asymptotic analysis for wave-like equations through geometric optics (also known as the Wentzel-Kramers-Brillouin (WKB) method or ray-tracing, ) is nowadays a classical tool that has been developed in several directions. An incomplete biography on the topic includes . It is by now well-known that wave-type equations, in a local framework, have solutions that are localized near curves (t,‚ÄÜx(t)) in space-time, also called rays. These curves are, in the interior of the domain of definition of the equation, solutions of a Hamiltonian system of ordinary differential equations which involves the coefficients of the operator. When one of these trajectories hits the boundary of the domain it is reflected according to the classical laws of optics.
In the case of equation , since ùí´s‚ÄÑ=‚ÄÑi‚àÇt‚ÄÖ+‚ÄÖ(‚ÄÖ‚àí‚ÄÖdx‚ÄÜ2)s is a pseudo-differential operator with principal symbol ps(x,‚ÄÜt,‚ÄÜŒæ,‚ÄÜœÑ)=œÑ‚ÄÖ‚àí‚ÄÖ|Œæ|2s, the Hamiltonian system is given by $$\begin{aligned}
    \begin{cases}
        \dot{x}(\sigma) = \partial_\xi p_s = \pm 2s|\xi(\sigma)|^{2s-1}, &amp;amp; x(0)=x_0
        \\
        \dot{t}(\sigma) = \partial_\tau p_s = 1, &amp;amp; t(0)=0
        \\
        \dot{\xi}(\sigma) = -\partial_x p_s = 0, &amp;amp; \xi(0)=\xi_0
        \\
        \dot{\tau}(\sigma) = -\partial_t p_s =0, &amp;amp; \tau(0)=|\xi_0|^{2s}.
    \end{cases}\end{aligned}$$
In addition, this system can be easily solved explicitly, and we obtain the following expressions for the bicharacteristics $$\begin{aligned}
    \begin{cases}
        x(\sigma) = x_0 \pm 2s|\xi_0|^{2s-1}\sigma
        \\
        t(\sigma) = \sigma 
        \\
        \xi(\sigma) = \xi_0
        \\
        \tau(\sigma) = |\xi_0|^{2s}.
    \end{cases}\end{aligned}$$
In particular, the rays of ùí´s are given by the curves (t,‚ÄÜx0‚ÄÖ¬±‚ÄÖ2s|Œæ0|2s‚ÄÖ‚àí‚ÄÖ1t)‚àà(0,‚ÄÜ+‚àû)‚ÄÖ√ó‚ÄÖ‚Ñù. Notice that, as one expects since the operator has constant coefficients, these rays are straight lines.
The approach that we use for building localized solutions is quite standard. In particular, we look for quasi-solutions to introducing the ansatz $$\begin{aligned}
\label{ansatz}
    {u^{\,\varepsilon} }(x,t) = \varepsilon^s e^{i\left[\xi_0\varepsilon^{-1}x\,+\,|\xi_0|^{2s}\varepsilon^{-2s}t\right]}\sum_{j\geq 0}\varepsilon^{\frac{s}{2}j}a_j\left(x,\varepsilon^{\frac{3}{2}s}t\right),\end{aligned}$$ where the normalization constant Œµs is chosen asking that the function u‚ÄÜŒµ has Hs(‚Ñù)-norm of the order $\mathcal O(1)$. The identification of the aj-s is then carried out imposing $$\begin{aligned}
{\mathcal{P} }_s{u^{\,\varepsilon} } = O(\varepsilon^{\infty}), \end{aligned}$$ thus obtaining a series of PDEs in which it is possible to clearly separate the leading order terms, with respect to Œµ, from several remainders which will vanish as Œµ‚ÄÑ‚Üí‚ÄÑ0. This generates a cascade system for the functions aj, which can then be determined as the solution of certain given Partial Differential Equations. In our case, the cascade system is the following one $$\begin{aligned}
\label{cascade_system}
    \begin{cases}
        i\partial_\tau a_0 + \mathcal{C}_{\frac s2} \mathcal{D}^{\frac{s}{2} } a_0 = 0  
        \\
        i\partial_\tau a_1 + \mathcal{C}_{\frac s2} \mathcal{D}^{\frac{s}{2} } a_1 + \mathcal{C}_{s} \mathcal{D}^{s} a_0 = 0  
        \\
        i\partial_\tau a_2 + \mathcal{C}_{\frac s2} \mathcal{D}^{\frac{s}{2} } a_2 + \mathcal{C}_{s} \mathcal{D}^{s} a_1 + \mathcal{C}_{\frac{3s}{2} } \mathcal{D}^{\frac{3s}{2} } a_0(\theta,\tau) = 0, &amp;amp; \displaystyle x\leq\theta\leq x+\frac{\varepsilon}{\xi_0}q
        \\
        i\partial_\tau a_j + \mathcal{C}_{\frac s2}\mathcal{D}^{\frac{s}{2} }a_j + \mathcal{C}_{s} \mathcal{D}^{s}a_{j-1} +  \mathcal{C}_{\frac{3s}{2} } \mathcal{D}^{\frac{3s}{2} }a_{j-2}(\theta,\tau) + {(-d_x^{\,2})^{s}a_{j-3} }, &amp;amp; j\geq 3
        \\
        &amp;amp;\displaystyle x\leq\theta\leq x+\frac{\varepsilon}{\xi_0}q.  
    \end{cases}\end{aligned}$$ with $\tau:=\varepsilon^{\frac 32 s}t$ and where ùíü‚ÄÜŒ≤ denotes the following fractional derivative of order Œ≤ $$\begin{aligned}
    \mathcal{D}^{\beta} f(x):= \frac{1}{\Gamma(1-\beta)}\int_{-\infty}^x \frac{f'(y)}{(x-y)^{\beta} }\,dy.\end{aligned}$$
Moreover, is uniquely solvable with initial conditions imposed at œÑ‚ÄÑ=‚ÄÑ0 and this, of course, allows to identify the expressions of the functions aj. See  for more details.
To the best of our knowledge, a WKB approach has not yet been fully developed in a non-local setting, and our work represents a first step in this direction, providing a complete procedure for obtaining a WKB expansion of equation .
Our study is motivated by control problems. Indeed, it is by now well-known that geometric optics constructions for wave-like equations can be used for deriving controllability properties. These properties are usually formulated by means of an observability inequality, in which the total energy of the solutions is uniformly estimated by a partial measurement (typically, the portion of energy localized in a subset of the domain or of its boundary). In this framework, the existence of localized solutions gives sharp necessary conditions for the observability property to hold. In fact, as it was remarked by Ralston in , in order to observe these solutions the observation set must intersect every ray. If this were not the case, one could construct a quasi solution along a ray that would not hit the observation set and which, being negligible outside an arbitrarily small neighborhood of the ray, could not be observed. This is the so-called Geometric Control Condition (GCC), which has been proved to be almost sufficient by Bardos, Lebeau and Rauch in , and necessary by Burq and G√©rard in .
In the particular case under analysis, we are able to show that given a ray (t,‚ÄÜx(t)) it is possible to construct quasi-solutions of the fractional Schrdinger equation such that the amount of their energy outside a ball of radius $\varepsilon^{\frac 14}$ centered at x(t) is of the order of $\varepsilon^{\frac 14}$. In more detail, we can show that if ${u_{\textrm{\small in} }}\in L^2({\mathbb{R} })$ and u‚ÄÜŒµ is constructed employing the expansion , then

The functions u‚ÄÜŒµ are approximate solutions to : on the one hand, we have that the function z‚ÄÜŒµ a time t‚ÄÑ=‚ÄÑ0 is close, in the L2-norm, to any initial datum in the form $$\begin{aligned}
        { {\left\|u_0(x)-{u^{\,\varepsilon} }(x,0)\right\|}_{L^2({\mathbb{R} })} } = \mathcal{O}(\varepsilon^{\frac{1}{2} }),\;\;\;\forall\varepsilon&amp;gt;0.
    \end{aligned}$$ On the other hand, the same remains true also for any other time t‚ÄÑ&amp;gt;‚ÄÑ0. $$\begin{aligned}
        { {\left\|u(x,t)-{u^{\,\varepsilon} }(x,t)\right\|}_{L^2({\mathbb{R} })} } = \mathcal{O}(\varepsilon^{\frac 12}),\;\;\;\forall\varepsilon&amp;gt;0.
    \end{aligned}$$ Hence, the functions z‚ÄÜŒµ are really approximating the real solution of in a L2-setting.
The initial energy of z‚ÄÜŒµ, i.e. the energy at t‚ÄÑ=‚ÄÑ0, is essentially bounded, up to some small reminder which vanishes as Œµ‚ÄÑ‚Üí‚ÄÑ0+: $$\begin{aligned}
        { {\left\|{u^{\,\varepsilon} }(x,0)\right\|}_{H^s({\mathbb{R} })} }^2 \approx 1.
    \end{aligned}$$ Also in this case, as a consequence of the energy conservation property, this same fact remains true for all times t‚ÄÑ&amp;gt;‚ÄÑ0.
The amount of energy away from a given ray (t,‚ÄÜx(t)) of size $\varepsilon^{\frac 14}$ is of the same order $\varepsilon^{\frac 14}$: $$\begin{aligned}
        \int_{|x-x(t)|&amp;gt;\varepsilon^{\frac 14} } \left|{(-d_x^{\,2})^{\frac s2}{u^{\,\varepsilon} }}(x,t)\right|^2\,dx = \mathcal O(\varepsilon^{\frac 14}).
    \end{aligned}$$ In other words, the energy which is not concentrated along x(t) is negligible with respect to the total amount. This third property is possibly the most important one, since it justify the possibility to analyze the propagation of the solutions of simply in terms of the propagation of the rays.

In order to study propagation properties of the solutions of our equation in terms of the propagation of the rays, a very important quantity that we shall consider is the so-called group velocity, which can be easily computed as $$\begin{aligned}
    v=\left|\frac xt\right| = 2s\varepsilon^{1-2s}|\xi_0|^{2s-1}.\end{aligned}$$
From the above formula we immediately see that, for s‚ÄÑ=‚ÄÑ1/2 we have v‚ÄÑ=‚ÄÑ1, i.e. the velocity is constant and independent of the frequency Œæ0. For s‚ÄÑ‚àà‚ÄÑ(0,‚ÄÜ1/2), instead, we have that 1‚ÄÖ‚àí‚ÄÖ2s‚ÄÑ&amp;gt;‚ÄÑ0. Hence, taking Œµ‚ÄÑ&amp;lt;‚ÄÑ1, we easily get $$\begin{aligned}
    v&amp;lt;|\xi_0|^{2s-1}.\end{aligned}$$ Finally, for s‚ÄÑ‚àà‚ÄÑ(1/2,‚ÄÜ1) the situation is the opposite. We have 1‚ÄÖ‚àí‚ÄÖ2s‚ÄÑ&amp;lt;‚ÄÑ0 and, for Œµ‚ÄÑ&amp;lt;‚ÄÑ1, $$\begin{aligned}
    v&amp;gt;|\xi_0|^{2s-1}.\end{aligned}$$
Hence, we can conclude that for s‚ÄÑ&amp;gt;‚ÄÑ1/2 the group velocity increases with the frequency and that the high frequency solutions are traveling faster and faster. On the other hand, for s‚ÄÑ&amp;lt;‚ÄÑ1/2, the group velocity decreases with the frequency the high frequency solutions are traveling and slower and slower. This behavior has then consequences from the point of view of observation properties for the solutions and, in particular, it confirms the already known results presented in .

</description>
        <pubDate>Mon, 01 Oct 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/posts/WBK-Expasion</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/posts/WBK-Expasion</guid>
        
        
        <category>Posts</category>
        
      </item>
    
      <item>
        <title>Control of PDEs involving non-local terms</title>
        <description>Why non-local?

Relevant models in Continuum Mechanics, Mathematical Physics and Biology are of non-local nature:

  Boltzmann equations in gas dynamics;
  Navier-Stokes equations in Fluid Mechanics;
  Keller-Segel model for Chemotaxis.


Moreover, these models are applied for the description of several complex phenomena for which a local approach is inappropriate or limiting.

  Peierls-Nabarro equation in elasticity 1
  Image processing algorithms 2
  Anomalous diffusion models 3
  Finance: description of the pricing of American options 4.


In this setting, classical PDE theory fails because of non-locality. Yet many of the existing techniques can be tuned and adapted, although this is often a delicate matter.

Fractional time derivatives

We analyze the problem of controllability of fractional (in time) Partial Differential Equations. In contrast with the classical PDE control theory, when driving these systems to rest, one is required not only to control the value of the state at the final time but also the memory accumulated by the long-tail effects that the fractional derivative introduces.
As a consequence, the notion of null controllability to equilibrium needs to take into account both the state and the memory term.

In particular, in (5) we consider the full controllability problem for the system



with $A$ and $B$ two linear unbound operators and where $\partial_{t,0+}^{\alpha}$ is a classical Caputo derivative. We show that, due to the memory effects controllability cannot be achieved in finite time.

Fractional Schr√∂dinger and wave equations

In 6, we analyse evolution problems involving the fractional Laplace operator $(-\Delta)^s$, $s\in(0,1)$, on a bounded $C^{1,1}$ domain $\Omega\subset\mathbb{R}^N$. In particular we consider the fractional Schr√∂dinger equation



and the fractional wave equation



Our main goal is to study the controllability of this kind of phenomena.


  Fractional Schr√∂dinger equation: we assume Dirichlet homogeneous boundary conditions and we prove null controllability provided $s\geq 1/2$ and that the control is active on a neighborhood $\omega$ of a subset of the boundary fulfilling the classical multiplier conditions. Moreover, in the limit case $s=1/2$, controllability holds only if the control time $T$ is large enough.
  Fractional wave equation: we obtain analogous controllability properties, as a direct consequence of the results for the Schr√∂dinger equation.


These mentioned controllability properties may be studied also through the employment of microlocal analysis techniques. In this framework, in 7 we present a WKB expansion for the fractional Schr√∂dinger equaiton in one space dimension, which allows to show that the solutions of the model propagate following the rays of geometric optics and that their controllability properties may be reinterpreted in terms of the velocity of propagation of this mentioned rays.

Viscoelasticity

Viscoelastic materials are those for which the behaviour combines liquid-like and solid-like characteristic 8.







In 9, we consider the following model of viscoelasticity, given by a wave equation with both a viscous Kelvin-Voigt and frictional damping



in which we incorporate an internal control $h$ with a moving support. Our analysis is based on the fact that the above equation can be rewritten as a system coupling a parabolic equation with an ODE. The presence of this ODE, in the case of a fixed  support of the control, is responsible for the lack of controllability of the system, due to the absence of propagation in the space-like direction. Therefore, we prove the null controllability when the control region, driven by the flow of an ODE, covers all the domain.

Equations with non-local spatial terms

In 10, we study the null controllability of the following linear heat equation with spatial non-local integral terms:



Under some analyticity assumptions on the corresponding kernel, we show that the equations is controllable. We employ compactness-uniqueness arguments in a suitable functional setting, an argument that is harder to apply for heat equations because of its very strong time irreversibility.

These mentioned results have been later improved in [], where we adopted a Carleman approach which allowed us to remove the (strong) analitycity assumptions on the kernel, and to replace them with sharp exponential decay conditions at the extrema of the time domain considered.

Models with memory terms

In 11, we approach the control problem for the following heat equation with lower order memory



This can be rewritten as a system coupling a heat equation with an ordinary differential equation, as in the context of viscoelasticity. In view of this structure we introduce a Moving geometric Control Condition (MGCC), which turns out to be sufficient for moving control. Furthermore, in (10) we obtain similar results for the following wave equation with a lower order memory term



while in 7 we were able to address the case of the following one-dimensional wave equation with memory in the principal part


Perspectives


  Weakening the MGCC for the control of viscoelasticity models.
  Models with fractional time derivatives: what kind of control theoretical properties can be expected once exact controllability is excluded?
  General analytic memory kernels.
  Geometric Optics for wave-like models involving the fractional Laplacian.
  Can Carleman inequalities handle non-local terms?
  Links with delay systems.
  Nonlinear models.


References


  
    
      G.~Lu - ‚ÄúThe Peierls-Nabarro model of dislocations: a venerable theory and its current development‚Äù - Handbook of Materials Modeling, 2005¬†&amp;#8617;
    
    
      G.~Gilboa and S.~Osher - ‚ÄúNonlocal operators with applications to image processing‚Äù - Multiscale Model Simul., 2008¬†&amp;#8617;
    
    
      M.~Bologna, C.~Tsallis and P. Grigolini - ‚ÄúAnomalous diffusion associated with nonlinear fractional derivative Fokker-Plank-like equation: exact time-time dependent solutions‚Äù - Phys. Rev., 2000¬†&amp;#8617;
    
    
      S.~Levendorski - ‚ÄúPricing of the American put under Levy processes‚Äù - Int. J. Theor. Appl. Finance, 2004¬†&amp;#8617;
    
    
      Q.~L&quot;u and E. Zuazua - ‚ÄúOn the lack of controllability of fractional in time ODE and PDE‚Äù - Math. Control Signals Syst., 2016¬†&amp;#8617;
    
    
      U.~Biccari - ‚ÄúInternal control for non-local Schr&quot;odinger and wave eqautions involving the fractional Laplace operator‚Äù - Submitted¬†&amp;#8617;
    
    
      U.~Biccari, and Aceves, A. B. WKB expansion for a fractional Schr√∂odinger equation with applications to controllability. Submitted. (2018).¬†&amp;#8617;¬†&amp;#8617;2
    
    
      T.H.~Banks, S.~Hu and Z.R.~Kenz - ‚ÄúA brief review of elasticity and viscoelasticity for solids} - Adv. Appl. Math. Mech., 2011.‚Äù¬†&amp;#8617;
    
    
      F.~Chaves, L.~Rosier and E.~Zuazua - ‚ÄúNull controllability of a system of viscoelasticity with moving control‚Äù - J. Math. Pures Appl., 2014.¬†&amp;#8617;
    
    
      Q.~L&quot;u, X.~Zhang and E. Zuazua - ‚ÄúNull controllability of wave equations with memory‚Äù - Submitted¬†&amp;#8617;¬†&amp;#8617;2
    
    
      F.~Chaves, X.~Zhang and E.~Zuazua - ‚ÄúControllability of evolution equations with memory‚Äù - Submitted¬†&amp;#8617;
    
  

</description>
        <pubDate>Mon, 01 Oct 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/posts/Control_of_PDE_non_local_terms</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/posts/Control_of_PDE_non_local_terms</guid>
        
        
        <category>Posts</category>
        
      </item>
    
      <item>
        <title>Rays propagation of a fractional Schrodinger Equation</title>
        <description>Shows the propagation of the solution of a fractional Schrodinger equation with concentrated and highly oscillatory initial datum. The solution remains concentrated along the rays of geometric optics

N = 250;
L = 1;
hx = (2*L)/(N+1);


Definition of the initial datum u0 as a function_handle. u0 is chosen as a Gaussian profile multiplied by a higly oscillatory function

x0 = 0; % Center of the Gaussian profile
gamma = hx^(-0.9); % Amplitude of the Gaussian profile
fr = (1/hx)*pi^2/16; % Frequency of the oscillations


u0 = @(x) exp(-0.5*gamma*(x-x0).^2).*exp(1i*fr*x);


Plot of the initial datum

fig = gcf;
set(gcf,'Units','pixels','Position',[427 306 712 284])

x = -L:hx:L;

subplot(1,3,1) % Modulus
plot(x,abs(u0(x)))
title('|u_0(x)|')
xlabel('x'); ylabel('u(x)');

subplot(1,3,2) % Real part
plot(x,real(u0(x)))
title('real(u_0(x))')
xlabel('x'); ylabel('u(x)');

subplot(1,3,3) % Imaginary part
plot(x,imag(u0(x)))
title('img(u_0(x))')
xlabel('x'); ylabel('u(x)');

format_plot(fig)




Solution for s = 1/2

Define the characteristic parameters of the problem

s = 0.5  % Order of the fractional Laplacian
L        % Extrema of the space interval
N        % Number of points in the space mesh
T = 5    % Length of the time interval
u0       % The function_handle that we have showed before.



s =

    0.5000


L =

     1


N =

   250


T =

     5


u0 =

  function_handle with value:

    @(x)exp(-0.5*gamma*(x-x0).^2).*exp(1i*fr*x)




To solve the equation, we call the function fractional_schr. The solution of the equation is stored in the u variable.

[x,t,u] = fractional_schr(s,L,N,T,u0);


Now we can see a graphical interpretation

[X,T] = meshgrid(x,t);
%
clf
mesh(X,T,u');
format_plot(gcf);view(0,90)
xlabel('x'); ylabel('t'); title('Ray Evolution');




By typing ‚Äúanimation(x,t,u)‚Äù in the MATLAB console you can see the evolution in time of this wave.



</description>
        <pubDate>Sat, 21 Jul 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp05/P0001-T</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp05/P0001-T</guid>
        
        
        <category>Tutorials</category>
        
        <category>WP05</category>
        
      </item>
    
      <item>
        <title>Average Control by classical gradient step method</title>
        <description>In this work, we address the optimal control of parameter-dependent systems. We introduce the notion of averaged control in which the quantity of interest is the average of the states with respect to the parameter family 

In this case $\nu_i$ are:

nu = 1:0.5:6



nu =

  Columns 1 through 7

    1.0000    1.5000    2.0000    2.5000    3.0000    3.5000    4.0000

  Columns 8 through 11

    4.5000    5.0000    5.5000    6.0000




And save in K, the number of values

K = length(nu);


Where the finite dimensional linear control system is:



We can, define the initial state of all ode‚Äôs

N = 3; % dimension of vector state
x0 = ones(N, 1);


Also, need define a initial control, that will be evolve

dt = 0.02;
t0 = 0; T  = 1;
span = (t0:dt:T);
%
u = zeros(length(span),1);


%Moreover, we can define the matrix A's and B's, that determine the problem
Am = -triu(ones(N))



Am =

    -1    -1    -1
     0    -1    -1
     0     0    -1




Bm = zeros(N, 1);
Bm(N) = 1



Bm =

     0
     0
     1




So, we can create these edo‚Äôs in variable primal_odes.

primal_odes = zeros(1,K,'ode');
for index = 1:K
    A = Am + (nu(index) - 1 )*diag(diag(Am));
    %
    primal_odes(index) = ode(A,'B',Bm);
    % all have the same control
    primal_odes(index).u  = u;
    % time intervals
    primal_odes(index).span = span;
    % initial state
    primal_odes(index).x0 = x0;
end


So, we have a $K$ ordinary differential equations

primal_odes



primal_odes = 

  1x11 ode array with properties:

    A
    B
    u
    x0
    x
    span
    xend




To solve average control problem to x0; in this case:

xt = ones(N, 1)



xt =

     1
     1
     1




we can solve the minimization problem



We can use the classical gradient descent method based on the adjoint methodology, and obtain the corresponding adjoint system for 1,



The same way that before, we define the adjoints problems

adjoint_odes = zeros(1,K,'ode');
for index = 1:K
    A = primal_odes(index).A';
    adjoint_odes(index) = ode(A);
    % all have the same control
    adjoint_odes(index).u = u;
    % time intervals
    adjoint_odes(index).span = span;
end


However the initial state  adjoint_odes(index).x0 hasn‚Äôt been assign. This initial state will be assign in every step of solution.

To minimize the functional, $\mathcal{J}\left( u\right)$, we take the steepest descent direction given by



We process to solve the problem of classical gradient descent

gamma = 1;
beta  = 1e-3;
tol   = 1e-8;  % Tolerance
error = Inf;
MaxIter = 50;
iter = 0;
xhistory = {}; uhistory = {};  error_history = [];    % array here we will save the evolution of average vector states
while (error &amp;gt; tol &amp;amp;amp;&amp;amp;amp; iter &amp;lt; MaxIter)
    iter = iter + 1;
    % solve primal problem
    % ====================
    solve(primal_odes);
    % calculate mean state final vector of primal problems
    xMend = forall({primal_odes.xend},'mean');

    % solve adjoints problems
    % =======================
    % update new initial state of all adjoint problems
    for iode = adjoint_odes
        iode.x0 = -(xMend' - xt);
    end
    % solve adjoints problems with the new initial state
    solve(adjoint_odes);

    % update control
    % ===============
    % calculate mean state vector of adjoints problems
    pM = forall({adjoint_odes.x},'mean');
    pM = pM*Bm;

    % reverse adjoint variable
    pM = flipud(pM);
    % Control update
    u = primal_odes(1).u; % catch control currently
    Du = beta*u - pM;
    u = u - gamma*Du;
    % update control in primal problems
    for index = 1:K
        primal_odes(index).u = u;
    end
    % Control error
    % =============
    % Calculate area ratio  of Du^2 and u^2
    Au2   =  trapz(span,u.^2);
    ADu2  =  trapz(span,Du.^2);
    %

    error = sqrt(ADu2/Au2);
    % Save evolution
    xhistory{iter} = [ span',forall({primal_odes.x},'mean')];
    uhistory{iter} = [ span',u];
    error_history  = [ error_history, error];
end


The average control obtain is

plot(span,u)
xlabel('time');ylabel('u(t)')
format_plot(gcf)




Also, on average the objective [0 0 0] has been reached.

figure;
plot(iode.span,forall({primal_odes.x},'mean'))
xlabel('t');ylabel('x_{i}(t)')
legend(strcat('x_{',num2str((1:N)','%0.1d'),'}(t)'))
title('Evolution of cordinates of vector state.')
format_plot(gcf)




You can use the comand

animation_sol(xhistory,uhistory,'XLim',[-0.1 0.25],'ULim',[-0.5 0.0])

We can see 

If we analyze the evolution in the error, we can see that we should have stopped, in iteration 20.

plot(error_history,'-*')
title('Error Evolution')
ylabel('Error'); xlabel('Iterations')
format_plot(gcf)




References


  
    
      E. Zuazua (2014) Averaged Control. Automatica, 50 (12), p. 3077-3087.¬†&amp;#8617;
    
  

</description>
        <pubDate>Sat, 21 Jul 2018 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp01/P0001-T</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/tutorials/wp01/P0001-T</guid>
        
        
        <category>Tutorials</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>rigidity_fr_laplacian</title>
        <description>

Description

Summary of example objective

Section 1 Title

Description of first code block

a = 1;


Section 2 Title

Description of second code block

b = 2;


</description>
        <pubDate>Sat, 01 Apr 2017 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/functions/rigidity_fr_laplacian-F</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/functions/rigidity_fr_laplacian-F</guid>
        
        
        <category>Functions</category>
        
      </item>
    
      <item>
        <title>fractional_schr</title>
        <description>

Description

Solves the fractional Schrodinger equation



using FE for the approximation of the fractional Laplacian and Cranck-Nicholson for the time integration

For this example we choose following parameters

s = 0.5;
N = 100;
L = 1;


Execute the function

A = rigidity_fr_laplacian(s,L,N);


Can see graphically representation of matrix

figure(1)
mesh(A)
view(155,100)




</description>
        <pubDate>Sat, 01 Apr 2017 00:00:00 +0200</pubDate>
        <link>https://DeustoTech.github.io/dycon-platform-documentation/functions/fractional_schr-F</link>
        <guid isPermaLink="true">https://DeustoTech.github.io/dycon-platform-documentation/functions/fractional_schr-F</guid>
        
        
        <category>Functions</category>
        
      </item>
    
  </channel>
</rss>
